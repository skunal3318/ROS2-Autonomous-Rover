<p align="center">
  <img src="https://raw.githubusercontent.com/ros-infrastructure/artwork/master/ros_logo.svg" alt="ROS 2 Logo" width="450">
</p>

<p align="center">
  <img src="https://img.shields.io/badge/ROS2-Jazzy-blue" alt="ROS2 Jazzy">
  <img src="https://img.shields.io/badge/Simulation-Gazebo%20Sim-green" alt="Gazebo Sim">
  <img src="https://img.shields.io/badge/Perception-Camera--Based-orange" alt="Camera Based Perception">
  <img src="https://img.shields.io/badge/Status-Active-success" alt="Project Status">
</p>

# ROS 2 Camera-Based Autonomous Rover

**ROS2 Camera-Based Autonomous Rover** is a **simulation-first mobile robotics project** built using **ROS 2 (Jazzy)** and **Gazebo Sim (gz-sim)**.  
The project focuses on **vision-based perception**, **obstacle reasoning**, and **clean ROS 2 architecture**, with an emphasis on **explainable autonomy** rather than black-box navigation.

This repository is structured to scale from **pure visualization ‚Üí perception ‚Üí decision-making ‚Üí control**.

---

## üåü Why This Project?

This project was built to:

- Understand **camera-based perception pipelines**
- Practice **clean ROS 2 package separation**
- Develop **obstacle avoidance without LiDAR**
- Visualize **what the robot ‚Äúsees‚Äù and ‚Äúdecides‚Äù**
- Build autonomy incrementally instead of end-to-end black boxes

The rover operates in a simulated environment and detects obstacles **purely from RGB camera input**, making it suitable for low-cost real-world robots.

---

## üöó Rover Capabilities (Current)

- üé• **RGB Camera Integration**
- üëÅÔ∏è **Camera-Based Obstacle Detection**
- üìê **Region-of-Interest (ROI) Processing**
- üß± **Edge-Based Obstacle Reasoning**
- üß† **Left / Center / Right Obstacle Classification**
- üñºÔ∏è **Real-Time Perception Visualization**
- üß™ **Fully Simulated in Gazebo Sim**

> üöß Motion control and FSM-based navigation are intentionally **not enabled yet** to maintain perception clarity.

---

## üß† Perception Pipeline

The obstacle avoidance logic follows a transparent vision pipeline:

1. **RGB Image Acquisition**
2. **ROI Cropping (Front View)**
3. **Grayscale Conversion**
4. **Gaussian Blur**
5. **Canny Edge Detection**
6. **Spatial Analysis**
   - Left / Center / Right regions
7. **Obstacle Decision Output**
   - `LEFT`, `CENTER`, `RIGHT`, or `NONE`

This approach prioritizes **interpretability and debugging visibility**.

---

## üß© System Architecture

The project uses a **modular ROS 2 design**:

### üß± Simulation
- Gazebo Sim (gz-sim)
- Custom rover URDF/Xacro
- RGB camera sensor

### üß† ROS 2 Packages
- **four_wheel_description**
  - Robot model
  - Sensors
  - Gazebo integration
- **four_control**
  - Camera visualization
  - Obstacle perception logic
  - Decision reasoning (no control yet)

### üëÄ Visualization
- OpenCV windows (camera + ROI edges)
- Gazebo Sim GUI
- RViz2 (optional)

This separation ensures:
- Easy debugging
- Clear responsibility boundaries
- Smooth transition to real hardware later

---

## üß™ Obstacle Avoidance Logic (Camera-Based)

Obstacle detection is performed using **image structure**, not distance sensors.

Decision logic:

- **CENTER obstacle** ‚Üí highest priority
- **LEFT obstacle** ‚Üí free space likely on right
- **RIGHT obstacle** ‚Üí free space likely on left
- **NONE** ‚Üí clear path

The result is visualized directly on the processed image to ensure **decision correctness before control is enabled**.

---

## üõ†Ô∏è Tech Stack

- **ROS 2 Jazzy**
- **Gazebo Sim (gz-sim)**
- **Python (rclpy)**
- **OpenCV**
- **cv_bridge**
- **URDF / Xacro**
- **RViz2**

---

## ‚ñ∂Ô∏è How to Run

### Build the workspace
```bash
colcon build
source install/setup.bash

```

### Launch the rover in Gazebo
```bash
ros2 launch four_wheel_description rover.launch.py
```

### Run camera-based obstacle visualization
```bash
ros2 launch four_control visualization.launch.py
```

